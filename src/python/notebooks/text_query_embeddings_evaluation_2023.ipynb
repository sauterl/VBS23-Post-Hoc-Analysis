{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Processing text queries\n",
    "- code in this notebook predominantly analyze text queries submitted by participants to solve KIS tasks of VBS 2023\n",
    "- among others, this notebook can replicate the content of Table 2 and 3 as well as Figures 13-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "from notebooks.utils import compute_user_penalty, get_team_values_df\n",
    "from common.load import load_competition_data, process_team_logs\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "unknownRankLimit = 1000\n",
    "unknownRankValue = 2000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import common data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'config_vbs2023.yaml'\n",
    "\n",
    "# load competition data from dres files and auxiliary data (FPSs, sequences)\n",
    "comp_data = load_competition_data(config)\n",
    "\n",
    "# load the preprocessed query data\n",
    "dataset = pd.read_pickle(comp_data[\"config\"][\"processed_logs_outdir\"] + '/text_query_dataset.pkl')\n",
    "\n",
    "# valid teams\n",
    "team_order = ['vibro', 'VISIONE',  'vitrivr-VR', 'CVHunter', 'Verge']\n",
    "#team_order = ['vibro', 'VISIONE', 'VIREO' 'vitrivr-VR', 'CVHunter', 'vitrivr', 'Verge']\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating auxiliary variables\n",
    "- Query length and volume of words per query\n",
    "- Maybe also store information whether the query is temporal? Only HTW and VISIONE have obviouse temporal queries\n",
    "- Define visual vs textual tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"task_type\"] = \"visual\"\n",
    "dataset.loc[dataset.task.str.contains(\"kis-t\"),\"task_type\"] = \"textual\"\n",
    "\n",
    "dataset[\"QT\"] = \"Other\"\n",
    "dataset.loc[dataset.is_joint_embedding_text_query, \"QT\"] = \"Text\"\n",
    "\n",
    "dataset[\"QueryLen\"] = -1\n",
    "dataset[\"QueryWords\"] = -1\n",
    "\n",
    "dataset.loc[dataset[\"category\"]==\"TEXT\",\"QueryLen\"] = dataset.loc[dataset[\"category\"]==\"TEXT\",\"value\"].str.len()\n",
    "dataset.loc[dataset[\"category\"]==\"TEXT\",\"QueryWords\"] = dataset.loc[dataset[\"category\"]==\"TEXT\",\"value\"].str.split().str.len()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#append dummy values for too high ratings (have to be labeled in plots properly in charts)\n",
    "dataset.loc[dataset.rank_video > unknownRankLimit,\"rank_video\"] = unknownRankValue\n",
    "dataset.loc[dataset.rank_shot_margin_0 > unknownRankLimit,\"rank_shot_margin_0\"] = unknownRankValue\n",
    "dataset.loc[dataset.rank_shot_margin_5 > unknownRankLimit,\"rank_shot_margin_5\"] = unknownRankValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset.loc[dataset.QT == \"Other\"]).is_temporal_query.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "textData = dataset.loc[dataset[\"QT\"]==\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape, textData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "textualMaxTime = 420000\n",
    "visualMaxTime = 300000\n",
    "dataset.loc[((dataset.correct_submission_time_ms.isna())&(dataset.task_type==\"textual\")),\"correct_submission_time_ms\"] = textualMaxTime\n",
    "dataset.loc[((dataset.correct_submission_time_ms.isna())&(dataset.task_type==\"visual\")),\"correct_submission_time_ms\"] = visualMaxTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boundaries = [0,60,120,180,240,300,360,420]\n",
    "boundaries = [b*1000 for b in boundaries]\n",
    "valid_bins = []\n",
    "dataset[\"hist_bin\"] = 0\n",
    "\n",
    "for b in boundaries:\n",
    "    \n",
    "    if b > 0:\n",
    "        # checking whether the team was available throughout the whole period of the bin\n",
    "        dataset[\"valid_\"+str(b-60*1000)] = (dataset[\"correct_submission_time_ms\"]>=b)\n",
    "        valid_bins.append(\"valid_\"+str(b-60*1000))\n",
    "    dataset.loc[dataset.elapsed_since_task_start_ms >= b, \"hist_bin\"] = b\n",
    "    \n",
    "dataset.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Work with text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.QT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "stdf = dataset.loc[dataset.QT==\"Text\",'joint_text_embedding'].values\n",
    "stdf = np.stack(stdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedColnames = [\"f_\"+str(i) for i in range(stdf.shape[1])]\n",
    "dfEmbeds = pd.DataFrame(stdf, columns=embedColnames, index=dataset.loc[dataset.QT==\"Text\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointDF = pd.concat([dataset.loc[dataset.QT==\"Text\"], dfEmbeds], axis=1)\n",
    "jointDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointDF.task.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "def upper_tri_indexing(A):\n",
    "    m = A.shape[0]\n",
    "    r,c = np.triu_indices(m,1)\n",
    "    return A[r,c]\n",
    "\n",
    "def ILD(dataset, columns):\n",
    "    dt = dataset[columns].values\n",
    "    if len(dt)==0:\n",
    "        return (np.empty(shape=(0, 0)), 0)\n",
    "    distMatrix = pairwise_distances(dt,metric=\"cosine\")\n",
    "    #remove distances to self\n",
    "    distMatrix = upper_tri_indexing(distMatrix)\n",
    "    return (distMatrix,distMatrix.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_tri_indexing(np.array([[1,2,3],[4,5,6],[7,8,9]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does query distances differ for individual tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats0 = {}\n",
    "for t in jointDF.task.unique():    \n",
    "    distMatrix, meanVal = ILD(jointDF.loc[jointDF[\"task\"]==t],embedColnames)\n",
    "    dMats0[t] = distMatrix.ravel()\n",
    "    print (t, meanVal)\n",
    "dMats0 = pd.Series(dMats0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textTasks = [i for i in jointDF.task.unique() if \"kis-t\" in i]\n",
    "visualTasks = [i for i in jointDF.task.unique() if ((\"kis-v-\" not in i)&(\"kis-v\" in i))]\n",
    "marineTasks = [i for i in jointDF.task.unique() if \"kis-v-\" in i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual tasks have smaller between-query distances than both visual ones\n",
    "- also marine tasks has slightly smaller distances than V3C1 visual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.concatenate(dMats0[textTasks].values).mean())\n",
    "print(np.concatenate(dMats0[visualTasks].values).mean())\n",
    "print(np.concatenate(dMats0[marineTasks].values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(dMats0[textTasks].values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "print(ttest_ind(np.concatenate(dMats0[textTasks].values),np.concatenate(dMats0[visualTasks].values)))\n",
    "print(ttest_ind(np.concatenate(dMats0[textTasks].values),np.concatenate(dMats0[marineTasks].values)))\n",
    "print(ttest_ind(np.concatenate(dMats0[marineTasks].values),np.concatenate(dMats0[visualTasks].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pd.DataFrame({\"v\": np.concatenate(dMats0[textTasks].values)})\n",
    "txt[\"type\"]=\"Textual\"\n",
    "vis = pd.DataFrame({\"v\": np.concatenate(dMats0[visualTasks].values)})\n",
    "vis[\"type\"]=\"Visual\"\n",
    "mar = pd.DataFrame({\"v\": np.concatenate(dMats0[marineTasks].values)})\n",
    "mar[\"type\"]=\"Marine Visual\"\n",
    "dfPlot = pd.concat([txt,vis,mar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(y=dfPlot[\"v\"],x=dfPlot[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar[\"v\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team-wise differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats = {}\n",
    "for t in jointDF.task.unique():\n",
    "    for tm in jointDF.team.unique():\n",
    "        distMatrix, meanVal = ILD(jointDF.loc[((jointDF[\"task\"]==t)&(jointDF[\"team\"]==tm))],embedColnames)\n",
    "        dMats[(t,tm)] = distMatrix.ravel()\n",
    "        print (t, tm, meanVal)\n",
    "dMats = pd.Series(dMats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tm in jointDF.team.unique():\n",
    "    keys = list(np.broadcast(jointDF.task.unique(),tm))\n",
    "\n",
    "    print(tm, np.concatenate(dMats[keys].values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = []\n",
    "team = []\n",
    "distance = []\n",
    "\n",
    "for t in jointDF.task.unique():\n",
    "    for tm in jointDF.team.unique():\n",
    "        #print(t,tm,np.mean(dMats[(t,tm)]))\n",
    "        for d in dMats[(t,tm)]:\n",
    "            task.append(t)\n",
    "            team.append(tm)\n",
    "            distance.append(d)\n",
    "dfGraph2 = pd.DataFrame({\"task\":task, \"team\":team,\"distance\":distance})\n",
    "dfGraph2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGraph2 = dfGraph2.sort_values(\"task\")\n",
    "dfGraph2\n",
    "sns.pointplot(data=dfGraph2, x=\"task\", y=\"distance\", hue=\"team\", markers=[\"o\",\"v\",\"s\",\"p\",\"*\"], errorbar=None, linestyles=\"dotted\")\n",
    "sns.pointplot(data=dfGraph2, x=\"task\", y=\"distance\", markers=[\"o\",\"v\",\"s\",\"p\",\"*\"], errorbar=None)\n",
    "\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = []\n",
    "team = []\n",
    "distance = []\n",
    "\n",
    "for t in jointDF.task.unique():\n",
    "    for tm in jointDF.team.unique():\n",
    "        #print(t,tm,np.mean(dMats[(t,tm)]))\n",
    "        task.append(t)\n",
    "        team.append(tm)\n",
    "        \n",
    "        distance.append(dMats[(t,tm)].mean())\n",
    "dfGraph = pd.DataFrame({\"task\":task, \"team\":team,\"mean distance\":distance})\n",
    "dfGraph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGraph = dfGraph.sort_values(\"task\")\n",
    "dfGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGraph[\"task\"] = dfGraph[\"task\"].str.replace(\"vbs23-\",\"\")\n",
    "dfGraph[\"task\"] = dfGraph[\"task\"].str.replace(\"kis-v-m\",\"KIS-V-M\")\n",
    "dfGraph[\"task\"] = dfGraph[\"task\"].str.replace(\"kis-v\",\"KIS-V\")\n",
    "dfGraph[\"task\"] = dfGraph[\"task\"].str.replace(\"kis-t\",\"KIS-T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = ['vibro', 'VISIONE', \"vitrivr-VR\",\"CVHunter\",'Verge']\n",
    "txtTasks = list(itertools.product(textTasks,hue_order))\n",
    "marTasks = list(itertools.product(marineTasks,hue_order))\n",
    "visTasks = list(itertools.product(visualTasks,hue_order))\n",
    "#val = np.concatenate(dMats[txtTasks].values).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for k in txtTasks:\n",
    "    try:\n",
    "        res.extend(dMats[k])\n",
    "    except:\n",
    "        print(\"not found key\",k)\n",
    "txtMean = np.mean(res)\n",
    "txtAll = res\n",
    "print(len(res),np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for k in marTasks:\n",
    "    try:\n",
    "        res.extend(dMats[k])\n",
    "    except:\n",
    "        print(\"not found key\",k)\n",
    "marMean = np.mean(res)\n",
    "marAll = res\n",
    "print(len(res),np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for k in visTasks:\n",
    "    try:\n",
    "        res.extend(dMats[k])\n",
    "    except:\n",
    "        print(\"not found key\",k)\n",
    "visMean = np.mean(res)\n",
    "visAll = res\n",
    "print(len(res),np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_ind(txtAll,marAll))\n",
    "print(ttest_ind(txtAll,visAll))\n",
    "print(ttest_ind(marAll,visAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team2color = {\n",
    "    'vibro'     : '#1f77b4',\n",
    "    'VISIONE'   : '#ff7f0e',\n",
    "    'VIREO'     : '#2ca02c',\n",
    "    'vitrivr-VR': '#d62728',\n",
    "    'CVHunter'  : '#9467bd',\n",
    "    'vitrivr'   : '#8c564b',\n",
    "    'Verge'     : '#e377c2',\n",
    "}\n",
    "\n",
    "team2marker = {\n",
    "    'vibro'     : 'D',\n",
    "    'VISIONE'   : 'X',\n",
    "    'VIREO'     : 'o', \n",
    "    'vitrivr-VR': '*',\n",
    "    'CVHunter'  : 'd',\n",
    "    'vitrivr'   : 'P',\n",
    "    'Verge'     : 's',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "plt.subplots(1,1,figsize=(8,4))\n",
    "dfGraph = dfGraph.sort_values(\"task\")\n",
    "hue_order = ['vibro', 'VISIONE', \"vitrivr-VR\",\"CVHunter\",'Verge']\n",
    "\n",
    "custom_palette = sns.color_palette(\"Set1\", 5)\n",
    "plt.axvline(x = 6.5, color=\"grey\", lw=0.5)\n",
    "plt.axvline(x = 12.5, color=\"grey\", lw=0.5)\n",
    "\n",
    "sns.scatterplot(data=dfGraph, x=\"task\", y=\"mean distance\", hue=\"team\", hue_order=hue_order, style=\"team\", palette=custom_palette)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y = txtMean, xmin = 0.01, xmax = 0.36, color=\"black\", linestyle = '--', label=\"Mean distance per task type\")\n",
    "plt.axhline(y = marMean, xmin = 0.38, xmax = 0.66, color=\"black\", linestyle = '--')\n",
    "plt.axhline(y = visMean, xmin = 0.68, xmax = 0.99, color=\"black\", linestyle = '--')\n",
    "\n",
    "#plt.axhline(y = np.concatenate(dMats0[textTasks].values).mean(), xmin = 0.01, xmax = 0.36, color=\"black\", linestyle = '--', label=\"Mean distance per task type\")\n",
    "#plt.axhline(y = np.concatenate(dMats0[marineTasks].values).mean(), xmin = 0.38, xmax = 0.66, color=\"black\", linestyle = '--')\n",
    "#plt.axhline(y = np.concatenate(dMats0[visualTasks].values).mean(), xmin = 0.68, xmax = 0.99, color=\"black\", linestyle = '--')\n",
    "plt.axhline(y = 0, xmin = 0.00, xmax = 0.00, color=\"black\", linestyle = ':', label=\"Mean distance per team\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for tm in ['vibro', 'VISIONE', \"vitrivr-VR\",\"CVHunter\",'Verge']:\n",
    "    keys = list(np.broadcast(jointDF.task.unique(),tm))\n",
    "    val = np.concatenate(dMats[keys].values).mean()\n",
    "    plt.axhline(y = val, xmin = 0.01, xmax = 0.99, color=custom_palette[i], linestyle = ':')\n",
    "    \n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"distances.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "plt.subplots(1,1,figsize=(8,4))\n",
    "dfGraph = dfGraph.sort_values(\"task\")\n",
    "hue_order = ['vibro', 'VISIONE', \"vitrivr-VR\",\"CVHunter\",'Verge']\n",
    "\n",
    "custom_palette = sns.color_palette(\"Set1\", 5)\n",
    "plt.axvline(x = 6.5, color=\"grey\", lw=0.5)\n",
    "plt.axvline(x = 12.5, color=\"grey\", lw=0.5)\n",
    "\n",
    "sns.scatterplot(data=dfGraph, x=\"task\", y=\"mean distance\", hue=\"team\", hue_order=hue_order, style=\"team\", palette=team2color, markers=team2marker)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(y = txtMean, xmin = 0.01, xmax = 0.36, color=\"black\", linestyle = '--', label=\"Mean distance per task type\")\n",
    "plt.axhline(y = marMean, xmin = 0.38, xmax = 0.66, color=\"black\", linestyle = '--')\n",
    "plt.axhline(y = visMean, xmin = 0.68, xmax = 0.99, color=\"black\", linestyle = '--')\n",
    "\n",
    "plt.axhline(y = 0, xmin = 0.00, xmax = 0.00, color=\"black\", linestyle = ':', label=\"Mean distance per team\")\n",
    "\n",
    "\n",
    "j=0\n",
    "posStart = [0.01, 0.385, 0.685]\n",
    "posEnd = [0.365, 0.665, 0.99]\n",
    "for tasks in [textTasks,marineTasks,visualTasks]:\n",
    "    print(tasks)\n",
    "    i = 0\n",
    "    for tm in ['vibro', 'VISIONE', \"vitrivr-VR\",\"CVHunter\",'Verge']:\n",
    "        keys = list(np.broadcast(tasks,tm))\n",
    "        val = np.concatenate(dMats[keys].values).mean()\n",
    "        plt.axhline(y = val, xmin = posStart[j], xmax = posEnd[j], color=team2color[tm], linestyle = ':')\n",
    "\n",
    "        i=i+1\n",
    "    j = j+1\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"distances_v2.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both vibro and VISIONE had more consistent per-task queries than CVHunter\n",
    "- comparison with other teams omitted due to missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_ind(np.concatenate(dMats[list(np.broadcast(jointDF.task.unique(),\"vibro\"))].values),np.concatenate(dMats[list(np.broadcast(jointDF.task.unique(),\"CVHunter\"))].values)))\n",
    "print(ttest_ind(np.concatenate(dMats[list(np.broadcast(jointDF.task.unique(),\"VISIONE\"))].values),np.concatenate(dMats[list(np.broadcast(jointDF.task.unique(),\"CVHunter\"))].values)))\n",
    "print(ttest_ind(np.concatenate(dMats[list(np.broadcast(jointDF.task.unique(),\"vibro\"))].values),np.concatenate(dMats[list(np.broadcast(jointDF.task.unique(),\"VISIONE\"))].values)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of both users per team and task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointDF.groupby([\"team\",\"user\"]).count()[\"task\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- removing verge as no distinction between users is available\n",
    "- several times, vitrivr-VR does not have any records from one of the users (candidate for removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointDFNoVerge = jointDF.loc[jointDF.team != \"Verge\"]\n",
    "jointDF.shape,jointDFNoVerge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ILD_pair(dataset1, dataset2, columns):\n",
    "    dt1 = dataset1[columns].values\n",
    "    dt2 = dataset2[columns].values\n",
    "    if (len(dt1)==0)|(len(dt2)==0):\n",
    "        return (np.empty(shape=(0, 0)), 0)\n",
    "    distMatrix = pairwise_distances(dt1,dt2,metric=\"cosine\")\n",
    "    return (distMatrix.flatten(),distMatrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats = {}\n",
    "for t in jointDFNoVerge.task.unique():\n",
    "    for tm in jointDFNoVerge.team.unique():\n",
    "        dt1 = jointDFNoVerge.loc[((jointDFNoVerge[\"task\"]==t)&(jointDFNoVerge[\"team\"]==tm)&(jointDFNoVerge[\"user\"]==0))]\n",
    "        dt2 = jointDFNoVerge.loc[((jointDFNoVerge[\"task\"]==t)&(jointDFNoVerge[\"team\"]==tm)&(jointDFNoVerge[\"user\"]==1))] \n",
    "        \n",
    "        distMatrix, meanVal = ILD_pair(dt1, dt2, embedColnames)\n",
    "        dMats[(t,tm)] = distMatrix.ravel()\n",
    "        print (t, tm, meanVal)\n",
    "dMats = pd.Series(dMats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for tm in jointDFNoVerge.team.unique():\n",
    "    keys = list(np.broadcast(jointDFNoVerge.task.unique(),tm))\n",
    "    print(tm, np.concatenate(dMats[keys].values).mean())\n",
    "    vals.extend(np.concatenate(dMats[keys].values))\n",
    "np.mean(vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not so much different from the results of the overall distances (just a bit higher values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats2 = {}\n",
    "for t in jointDFNoVerge.task.unique():\n",
    "    for tm in jointDFNoVerge.team.unique():\n",
    "        for u in jointDFNoVerge.user.unique():\n",
    "            distMatrix, meanVal = ILD(jointDFNoVerge.loc[((jointDFNoVerge[\"task\"]==t)&(jointDFNoVerge[\"team\"]==tm)&(jointDFNoVerge[\"user\"]==u))],embedColnames)\n",
    "            dMats2[(t,tm,u)] = distMatrix.ravel()\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats2 = pd.Series(dMats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=[]\n",
    "for tm in jointDFNoVerge.team.unique():\n",
    "    keys = list(np.broadcast(jointDFNoVerge.task.unique(),tm))\n",
    "    users = [0]*len(keys)+[1]*len(keys)\n",
    "    keys = [(keys[i%len(keys)][0],keys[i%len(keys)][1],val) for i,val in enumerate(users)]\n",
    "    print(tm, np.concatenate(dMats2[keys].values).mean())\n",
    "    vals.extend(np.concatenate(dMats2[keys].values))\n",
    "np.mean(vals)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- quite a few NaNs due to having only a single query per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats3 = {}\n",
    "for t in jointDFNoVerge.task.unique():\n",
    "    for tm in jointDFNoVerge.team.unique():\n",
    "        dt1 = jointDFNoVerge.loc[((jointDFNoVerge[\"task\"]==t)&(jointDFNoVerge[\"team\"]==tm))]\n",
    "        \n",
    "        for tm2 in jointDFNoVerge.team.unique():\n",
    "            if tm2!= tm:\n",
    "                dt2_1 = jointDFNoVerge.loc[((jointDFNoVerge[\"task\"]==t)&(jointDFNoVerge[\"team\"]==tm2))]                                \n",
    "                distMatrix, meanVal = ILD_pair(dt1, dt2_1, embedColnames)\n",
    "                dMats3[(t,tm,tm2)] = distMatrix.ravel()\n",
    "                print (t, tm,tm2, meanVal)\n",
    "dMats3 = pd.Series(dMats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valsA = []\n",
    "for tm in jointDFNoVerge.team.unique():\n",
    "    vals = []\n",
    "    for tm2 in jointDFNoVerge.team.unique():\n",
    "        if tm != tm2:\n",
    "            keys = list(np.broadcast(jointDFNoVerge.task.unique(),tm, tm2))\n",
    "            users = [0]*len(keys)+[1]*len(keys)\n",
    "            keys = [(keys[i%len(keys)][0],keys[i%len(keys)][1],keys[i%len(keys)][2],val) for i,val in enumerate(users)]\n",
    "            print(tm, tm2, np.concatenate(dMats3[keys].values).mean())\n",
    "\n",
    "            vals.extend(np.concatenate(dMats3[keys].values))\n",
    "            valsA.extend(np.concatenate(dMats3[keys].values)) \n",
    "    print(tm,np.mean(vals))\n",
    "    print()\n",
    "    \n",
    "print(np.mean(valsA))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much smaller differences in within-user query distance as compared to between users (in the same team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tm in jointDFNoVerge.team.unique():\n",
    "    keys = list(np.broadcast(jointDFNoVerge.task.unique(),tm))\n",
    "    keys1 = keys\n",
    "    users = [0]*len(keys)+[1]*len(keys)\n",
    "    keys = [(keys[i%len(keys)][0],keys[i%len(keys)][1],val) for i,val in enumerate(users)]\n",
    "    \n",
    "    print(tm,ttest_ind(np.concatenate(dMats[keys1].values),np.concatenate(dMats2[keys].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in sequences of query reformulations\n",
    "- only users & tasks, where at least **4** text queries were made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "lastRank = 0\n",
    "lastRecord = (0,0,0)\n",
    "#TODO: only for textual reformulations\n",
    "sortedData = textData.sort_values([\"task\",\"team\",\"user\",\"timestamp\"])\n",
    "for idx, row in sortedData.iterrows():\n",
    "    record = (row[\"task\"],row[\"team\"],row[\"user\"])\n",
    "    if record != lastRecord:\n",
    "        lastRecord = record\n",
    "        lastRank = 0\n",
    "    lastRank += 1\n",
    "    ranks.append(lastRank)\n",
    "\n",
    "sortedData[\"QueryRank\"] = ranks\n",
    "\n",
    "#record how long was the interaction for each task and user\n",
    "querySeriesLen = sortedData.groupby([\"task\",\"team\",\"user\"])[[\"QueryRank\"]].max()\n",
    "querySeriesLen.columns = [\"MaxQueryRank\"]\n",
    "sortedData = sortedData.join(querySeriesLen, on=[\"task\",\"team\",\"user\"])\n",
    "sortedData[\"DiffFromMaxQueryRank\"] = sortedData.QueryRank - sortedData.MaxQueryRank\n",
    "sortedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = sortedData.loc[((sortedData.QueryRank <= 4)&(sortedData.MaxQueryRank >= 4))]\n",
    "#cannot be done for verge as only one user is present\n",
    "dt = dt.loc[dt.team != \"Verge\"]\n",
    "dt.columns,dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdf = dt.loc[dt.QT==\"Text\",'joint_text_embedding'].values\n",
    "stdf = np.stack(stdf)\n",
    "dfEmbeds = pd.DataFrame(stdf, columns=embedColnames, index=dt.loc[dt.QT==\"Text\"].index)\n",
    "seqDFEmbeds = pd.concat([dt.loc[dt.QT==\"Text\"], dfEmbeds], axis=1)\n",
    "seqDFEmbeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ILD_noRemove(dataset, columns):\n",
    "    dt = dataset[columns].values\n",
    "    if len(dt)==0:\n",
    "        return (np.empty(shape=(0, 0)), 0)\n",
    "    distMatrix = pairwise_distances(dt,metric=\"cosine\")\n",
    "    return (distMatrix,distMatrix.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dMats3 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = ILD_noRemove(dt,embedColnames)\n",
    "            if len(distMatrix)>0:\n",
    "                dMats3[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats3 = pd.Series(dMats3)\n",
    "sequentialResultsArray = np.stack(dMats3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequentialResultsArray[:,0,:].mean(axis=0)#distances to first query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances to subsequent queries\n",
    "print(\n",
    "    sequentialResultsArray[:,0,1].mean(),\n",
    "    sequentialResultsArray[:,1,2].mean(),\n",
    "    sequentialResultsArray[:,2,3].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([sequentialResultsArray[:,0,1],sequentialResultsArray[:,1,2],sequentialResultsArray[:,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While the distance to the initial query rises over time (unsurprisingly), the step size between consecutive queries remain roughly the same and rather small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import ratio\n",
    "def LevenshteinNormDist(dataset, txtCol):\n",
    "    dt = dataset[txtCol].values\n",
    "    if len(dt)==0:\n",
    "        return (np.empty(shape=(0, 0)), 0)\n",
    "    distMatrix = np.zeros((len(dt),len(dt)))\n",
    "    for i,t1 in enumerate(dt):\n",
    "        for j,t2 in enumerate(dt):\n",
    "            distMatrix[i,j] = 1 - ratio(t1,t2)\n",
    "\n",
    "    return (distMatrix,distMatrix.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats4 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = LevenshteinNormDist(dt,\"value\")\n",
    "            if len(distMatrix)>0:\n",
    "                dMats4[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats4 = pd.Series(dMats4)\n",
    "sequentialResultsLevensteinArray = np.stack(dMats4.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequentialResultsLevensteinArray[:,0,:].mean(axis=0)#distances to first query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances to subsequent queries\n",
    "print(\n",
    "    sequentialResultsLevensteinArray[:,0,1].mean(),\n",
    "    sequentialResultsLevensteinArray[:,1,2].mean(),\n",
    "    sequentialResultsLevensteinArray[:,2,3].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(sequentialResultsLevensteinArray[:,0,1], sequentialResultsLevensteinArray[:,1,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Levenstein distance support those of embeds distance. It seems that subsequent changes are a bit smaller for later reformulations, but no stat sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in sequences of query reformulations\n",
    "- only users & tasks, where at least **3** text queries were made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = sortedData.loc[((sortedData.QueryRank <= 3)&(sortedData.MaxQueryRank >= 3))]\n",
    "#cannot be done for verge as only one user is present\n",
    "dt = dt.loc[dt.team != \"Verge\"]\n",
    "\n",
    "stdf = dt.loc[dt.QT==\"Text\",'joint_text_embedding'].values\n",
    "stdf = np.stack(stdf)\n",
    "dfEmbeds = pd.DataFrame(stdf, columns=embedColnames, index=dt.loc[dt.QT==\"Text\"].index)\n",
    "seqDFEmbeds = pd.concat([dt.loc[dt.QT==\"Text\"], dfEmbeds], axis=1)\n",
    "seqDFEmbeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats3 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = ILD_noRemove(dt,embedColnames)\n",
    "            if len(distMatrix)>0:\n",
    "                dMats3[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats3 = pd.Series(dMats3)\n",
    "sequentialResultsArray = np.stack(dMats3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequentialResultsArray[:,0,:].mean(axis=0)) #distances to first query\n",
    "\n",
    "print(#distances to subsequent queries\n",
    "    sequentialResultsArray[:,0,1].mean(),\n",
    "    sequentialResultsArray[:,1,2].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(sequentialResultsArray[:,0,1], sequentialResultsArray[:,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats4 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = LevenshteinNormDist(dt,\"value\")\n",
    "            if len(distMatrix)>0:\n",
    "                dMats4[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats4 = pd.Series(dMats4)\n",
    "sequentialResultsLevensteinArray = np.stack(dMats4.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequentialResultsLevensteinArray[:,0,:].mean(axis=0)) #distances to first query\n",
    "\n",
    "print(#distances to subsequent queries\n",
    "    sequentialResultsLevensteinArray[:,0,1].mean(),\n",
    "    sequentialResultsLevensteinArray[:,1,2].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttest_ind(sequentialResultsLevensteinArray[:,0,1], sequentialResultsLevensteinArray[:,1,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in sequences of query reformulations\n",
    "- only users & tasks, where at least **5** text queries were made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = sortedData.loc[((sortedData.QueryRank <= 5)&(sortedData.MaxQueryRank >= 5))]\n",
    "#cannot be done for verge as only one user is present\n",
    "dt = dt.loc[dt.team != \"Verge\"]\n",
    "\n",
    "stdf = dt.loc[dt.QT==\"Text\",'joint_text_embedding'].values\n",
    "stdf = np.stack(stdf)\n",
    "dfEmbeds = pd.DataFrame(stdf, columns=embedColnames, index=dt.loc[dt.QT==\"Text\"].index)\n",
    "seqDFEmbeds = pd.concat([dt.loc[dt.QT==\"Text\"], dfEmbeds], axis=1)\n",
    "seqDFEmbeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats3 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = ILD_noRemove(dt,embedColnames)\n",
    "            if len(distMatrix)>0:\n",
    "                dMats3[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats3 = pd.Series(dMats3)\n",
    "sequentialResultsArray = np.stack(dMats3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequentialResultsArray[:,0,:].mean(axis=0)) #distances to first query\n",
    "\n",
    "print(#distances to subsequent queries\n",
    "    sequentialResultsArray[:,0,1].mean(),\n",
    "    sequentialResultsArray[:,1,2].mean(),\n",
    "    sequentialResultsArray[:,2,3].mean(),\n",
    "    sequentialResultsArray[:,3,4].mean(),\n",
    "    #sequentialResultsArray[:,4,5].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats4 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = LevenshteinNormDist(dt,\"value\")\n",
    "            if len(distMatrix)>0:\n",
    "                dMats4[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats4 = pd.Series(dMats4)\n",
    "sequentialResultsLevensteinArray = np.stack(dMats4.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequentialResultsLevensteinArray[:,0,:].mean(axis=0)) #distances to first query\n",
    "\n",
    "print(#distances to subsequent queries\n",
    "    sequentialResultsLevensteinArray[:,0,1].mean(),\n",
    "    sequentialResultsLevensteinArray[:,1,2].mean(),\n",
    "    sequentialResultsLevensteinArray[:,2,3].mean(),\n",
    "    sequentialResultsLevensteinArray[:,3,4].mean(),\n",
    "    #sequentialResultsLevensteinArray[:,4,5].mean()\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in sequences of query reformulations\n",
    "- only users & tasks, where at least **4** text queries were made\n",
    "- selecting last 4 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = sortedData.loc[((sortedData.DiffFromMaxQueryRank >= -3)&(sortedData.MaxQueryRank >= 4))]\n",
    "#cannot be done for verge as only one user is present\n",
    "dt = dt.loc[dt.team != \"Verge\"]\n",
    "\n",
    "stdf = dt.loc[dt.QT==\"Text\",'joint_text_embedding'].values\n",
    "stdf = np.stack(stdf)\n",
    "dfEmbeds = pd.DataFrame(stdf, columns=embedColnames, index=dt.loc[dt.QT==\"Text\"].index)\n",
    "seqDFEmbeds = pd.concat([dt.loc[dt.QT==\"Text\"], dfEmbeds], axis=1)\n",
    "seqDFEmbeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats3 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = ILD_noRemove(dt,embedColnames)\n",
    "            if len(distMatrix)>0:\n",
    "                dMats3[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats3 = pd.Series(dMats3)\n",
    "sequentialResultsArray = np.stack(dMats3.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequentialResultsArray[:,3,:].mean(axis=0)) #distances to first query\n",
    "\n",
    "print(#distances to subsequent queries\n",
    "    sequentialResultsArray[:,0,1].mean(),\n",
    "    sequentialResultsArray[:,1,2].mean(),\n",
    "    sequentialResultsArray[:,2,3].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(sequentialResultsArray[:,0,1], sequentialResultsArray[:,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([sequentialResultsArray[:,0,1],sequentialResultsArray[:,1,2],sequentialResultsArray[:,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMats4 = {}\n",
    "for t in seqDFEmbeds.task.unique():\n",
    "    for tm in seqDFEmbeds.team.unique():\n",
    "        for u in seqDFEmbeds.user.unique():\n",
    "            dt = seqDFEmbeds.loc[((seqDFEmbeds[\"task\"]==t)&(seqDFEmbeds[\"team\"]==tm)&(seqDFEmbeds[\"user\"]==u))]\n",
    "            dt = dt.sort_values(\"QueryRank\")\n",
    "            #print(dt.QueryRank)\n",
    "            distMatrix, meanVal = LevenshteinNormDist(dt,\"value\")\n",
    "            if len(distMatrix)>0:\n",
    "                dMats4[(t,tm,u)] = distMatrix\n",
    "            print (t, tm,u, meanVal)\n",
    "dMats4 = pd.Series(dMats4)\n",
    "sequentialResultsLevensteinArray = np.stack(dMats4.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequentialResultsLevensteinArray[:,0,:].mean(axis=0)) #distances to first query\n",
    "\n",
    "print(#distances to subsequent queries\n",
    "    sequentialResultsLevensteinArray[:,0,1].mean(),\n",
    "    sequentialResultsLevensteinArray[:,1,2].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttest_ind(sequentialResultsLevensteinArray[:,0,1], sequentialResultsLevensteinArray[:,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
